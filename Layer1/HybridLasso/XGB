import pandas as pd
from google.colab import drive
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from ipaddress import ip_address
import numpy as np

# Mount Google Drive
drive.mount('/content/drive')

# Load datasets
doh_data = pd.read_csv('/content/drive/MyDrive/Datasets/l1-doh.csv')
nondoh_data = pd.read_csv('/content/drive/MyDrive/Datasets/l1-nondoh.csv')

# Combine datasets
all_data = pd.concat([doh_data, nondoh_data])

# Handle missing values (replace with your actual handling strategy)
all_data.fillna(0, inplace=True)

# Convert IP addresses to numerical features
all_data['SourceIP_Num'] = all_data['SourceIP'].apply(lambda x: int(ip_address(x)))
all_data['DestinationIP_Num'] = all_data['DestinationIP'].apply(lambda x: int(ip_address(x)))

# Prepare features and labels
X = all_data.drop(['Label', 'TimeStamp', 'SourceIP', 'DestinationIP'], axis=1)  # Exclude original IP address columns
y = all_data['Label']

# Encode the target labels to numerical values
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Compute the correlation matrix
corr_matrix = X_train.corr().abs()  # Now the DataFrame contains only numerical data

# Identify features with a correlation above 0.9
upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]

# Drop the highly correlated features
X_train_filtered = X_train.drop(high_corr_features, axis=1)
X_test_filtered = X_test.drop(high_corr_features, axis=1)

# Update numeric features after dropping highly correlated ones
numeric_features_filtered = X_train_filtered.select_dtypes(include=['int64', 'float64']).columns

# Preprocessing pipelines for the filtered data
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# XGBoost classifier
xgb_classifier = XGBClassifier(random_state=42)

# Full pipeline with feature selection using Lasso and XGBoost classifier
lasso_xgb_pipeline_filtered = Pipeline(steps=[
    ('preprocessor', ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features_filtered)
        ])),
    ('feature_selection', SelectFromModel(Lasso(alpha=0.01, random_state=42))),
    ('classifier', xgb_classifier)
])

# Fit the model with filtered data
lasso_xgb_pipeline_filtered.fit(X_train_filtered, y_train)

# Predictions and evaluation
y_pred_xgb_filtered = lasso_xgb_pipeline_filtered.predict(X_test_filtered)

# Calculate accuracy
accuracy_xgb_filtered = accuracy_score(y_test, y_pred_xgb_filtered)
print(f'Accuracy for XGBoost with Hybrid Lasso after removing high correlation features: {accuracy_xgb_filtered:.4f}')

# Print classification report
print('Classification Report for XGBoost with Hybrid Lasso after removing high correlation features:')
print(classification_report(y_test, y_pred_xgb_filtered))

# Calculate confusion matrix and additional metrics
cm_xgb_filtered = confusion_matrix(y_test, y_pred_xgb_filtered)
tn, fp, fn, tp = cm_xgb_filtered.ravel()

# Number of features selected
num_selected_features = len(X_train_filtered.columns[lasso_xgb_pipeline_filtered.named_steps['feature_selection'].get_support()])

print(f'Number of Features Selected: {num_selected_features}')
print(f'False Positives: {fp}')
print(f'False Negatives: {fn}')

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb_filtered, annot=True, cmap='Blues', fmt='d',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix - XGBoost with Hybrid Lasso (Filtered)')
plt.show()

# Plot feature importances
feature_importances = lasso_xgb_pipeline_filtered.named_steps['classifier'].feature_importances_
features = X_train_filtered.columns

# Get the names of the selected features
selected_features = X_train_filtered.columns[lasso_xgb_pipeline_filtered.named_steps['feature_selection'].get_support()]

# Create a DataFrame for plotting using only the selected features
importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importances from XGBoost')
plt.show()
